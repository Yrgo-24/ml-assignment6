/**
 * @brief Dense layer stub.
 */
#pragma once

#include <stdexcept>

#include "ml/act_func/type.h"
#include "ml/dense_layer/interface.h"
#include "ml/types.h"
#include "ml/utils.h"

namespace ml::dense_layer::stub
{
/**
 * @brief Dense layer stub.
 * 
 *        This class is non-copyable and non-movable.
 */
class Dense final : public Interface
{
public:
    /**
     * @brief Create a new dense layer.
     * 
     * @param[in] inputSize Input size.
     * @param[in] outputSize Output size.
     * @param[in] actFunc Activation function to use for this layer (default = ReLU).
     */
    explicit Dense(const std::size_t inputSize, const std::size_t outputSize, 
                   const act_func::Type actFunc = act_func::Type::Relu)
        : myInputGradients{}
        , myBias{}
        , myWeights{}
        , myOutput{}
        , myError{}
    {
        // Throw exception if node count or the weight count is 0.
        if (0U == outputSize)
        {
            throw std::invalid_argument("Node count cannot be 0!");
        }
        else if (0U == inputSize)
        {
            throw std::invalid_argument("Weight count cannot be 0!");
        }

        // Initialize the matrices.
        initMatrix(myInputGradients, inputSize);
        initMatrix(myBias, outputSize);
        initMatrix(myWeights, outputSize, inputSize);
        initMatrix(myOutput, outputSize);
        initMatrix(myError, outputSize);

        // Ignore activation function in this implementation.
        (void) (actFunc);
    }

    /**
     * @brief Destructor.
     */
    ~Dense() noexcept override = default;

    /**
     * @brief Get the input size of the layer.
     * 
     * @return The input size of the layer.
     */
    std::size_t inputSize() const noexcept override 
    {
        return myWeights.empty() ? 0U : myWeights[0U].size(); 
    }

    /**
     * @brief Get the output size of the layer.
     * 
     * @return The output size of the layer.
     */
    std::size_t outputSize() const noexcept override { return myOutput.size(); }

    /**
     * @brief Get the output values of the layer.
     * 
     * @return Matrix holding the output values of the layer.
     */
    const Matrix1d& output() const noexcept override { return myOutput; }

    /**
     * @brief Get the input gradients of the layer.
     * 
     * @return Matrix holding the input gradients of the layer.
     */
    const Matrix1d& inputGradients() const noexcept override { return myInputGradients; }

    /**
     * @brief Perform feedforward operation.
     * 
     * @param[in] input Matrix holding input data.
     * 
     * @return True on success, false on failure.
     */
    bool feedforward(const Matrix1d& input) noexcept override
    {
        // Return true if the input dimensions match match.
        constexpr const char* opName{"feedforward in dense layer"};
        return matchDimensions(inputSize(), input.size(), opName);
    }

    /**
     * @brief Perform backpropagation.
     * 
     * @param[in] outputGradients Matrix holding gradients from the next layer.
     * 
     * @return True on success, false on failure.
     */
    bool backpropagate(const Matrix1d& outputGradients) noexcept override
    {
        // Return false if the output dimensions don't match.
        constexpr const char* opName{"backpropagation in output dense layer"};
        return matchDimensions(outputSize(), outputGradients.size(), opName);
    }

    /**
     * @brief Perform optimization.
     * 
     * @param[in] input Matrix holding input data.
     * @param[in] learningRate Learning rate to use.
     * 
     * @return True on success, false on failure.
     */
    bool optimize(const Matrix1d& input, const double learningRate) noexcept override
    {
        // Return true if the input dimensions match and the learning rate is valid.
        constexpr const char* opName{"optimization in dense layer"};
        return matchDimensions(inputSize(), input.size(), opName) 
            && checkLearningRate(learningRate, opName);
    }

    Dense()                        = delete; // No default constructor.
    Dense(const Dense&)            = delete; // No copy constructor.
    Dense(Dense&&)                 = delete; // No move constructor.
    Dense& operator=(const Dense&) = delete; // No copy assignment.
    Dense& operator=(Dense&&)      = delete; // No move assignment.

private:
    /** Input gradients. */
    Matrix1d myInputGradients;

    /** Bias values. */
    Matrix1d myBias;

    /** Weights for each node. */
    Matrix2d myWeights;

    /** Output matrix. */
    Matrix1d myOutput;

    /** Error values. */
    Matrix1d myError;
};
} // namespace ml::dense_layer::stub